{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functorch\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import simulation data\n",
    "u_array = np.load('unp_concat_100.npy')\n",
    "m_array = np.load('mnp_concat_100.npy')\n",
    "Jac_array = np.load('Jacnp_concat_100.npy')\n",
    "\n",
    "# Import reduced-order subspaces\n",
    "AS = np.load('AS_fs_wom.npy')\n",
    "PCA = np.load('PCA_U_f_100_allstep.npy')\n",
    "\n",
    "# Import finite element mass matrix\n",
    "M = np.load('M.npy')\n",
    "\n",
    "# Import Bayesian prior information\n",
    "prior_m_precision = np.load('prior_prec.npy')\n",
    "prior_m_covariance = np.load('prior.npy')\n",
    "prior_mean = np.load('prior_mean.npy')\n",
    "\n",
    "# Import reference solution data\n",
    "obs_mean = np.load('obs_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare parameter data\n",
    "m_array = torch.tensor(m_array)\n",
    "m_array_ = m_array - prior_mean  \n",
    "\n",
    "# Reduce parameter dimension using active subspace\n",
    "m_red = m_array_ @ (prior_m_precision @ AS)\n",
    "\n",
    "# Prepare observation data\n",
    "u_array_ = u_array - obs_mean \n",
    "\n",
    "# Truncate PCA basis to 129 dimensions\n",
    "PCA = PCA[:,:129]\n",
    "\n",
    "# Set number of time steps for training\n",
    "num_steps = 100\n",
    "\n",
    "# Create training datasets\n",
    "train_m = torch.tensor(m_red.numpy()[:-100], dtype=torch.float32)  \n",
    "train_s = torch.tensor(u_array_[:-100,0,:]@PCA, dtype=torch.float32)  \n",
    "output_s = torch.tensor(u_array_[:-100,1:1+num_steps,:]@PCA, dtype=torch.float32) \n",
    "\n",
    "# Normalize parameter data\n",
    "train_m_100 = train_m\n",
    "\n",
    "mean = torch.mean(train_m_100,dim=0)  \n",
    "std = torch.std(train_m_100,dim=0)    \n",
    "\n",
    "sdata_m = (train_m_100 - mean)/std   \n",
    "\n",
    "# Normalize observation data\n",
    "mean_o = output_s.mean(dim = (0,1))   \n",
    "# std_o = output_s.std(dim=(0,1))     \n",
    "\n",
    "# Set observation std to 1 for normalization\n",
    "std_o = torch.ones_like(std_o)\n",
    "\n",
    "sdata_y = (train_s - mean_o)/(std_o)      \n",
    "sdata_y_t = (output_s - mean_o)/(std_o)  \n",
    "\n",
    "# Normalize Jacobian data using same scaling\n",
    "Jac_r_ = torch.tensor(Jac_array)\n",
    "Jac_r = Jac_r_.transpose(2,3)         \n",
    "Jac_r = Jac_r[:,1:]                   \n",
    "Jac_r_n_1 = Jac_r / std_o.unsqueeze(1).unsqueeze(0).unsqueeze(0)  \n",
    "Jac_r_n_2 = Jac_r_n_1 * std.unsqueeze(0).unsqueeze(0).unsqueeze(0)  \n",
    "sdata_jac_ = Jac_r_n_2[:,:num_steps]  \n",
    "sdata_jac = sdata_jac_[:-100]         \n",
    "\n",
    "# Limit dataset size to 1280 samples\n",
    "sdata_m_ = sdata_m[:1024+256]\n",
    "sdata_y_ = sdata_y[:1024+256]\n",
    "sdata_y_t_ = sdata_y_t[:1024+256]\n",
    "sdata_jac_ = sdata_jac[:1024+256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from preprocessed tensors\n",
    "dataset = TensorDataset(sdata_m_, sdata_y_, sdata_y_t_, sdata_jac_)\n",
    "total_samples = len(dataset)\n",
    "\n",
    "# Define 80-20 train-validation split\n",
    "train_size = int(total_samples * 0.8)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "# Randomly split dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)   \n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attention layer for simplified version\n",
    "\n",
    "class DualPurposeAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Shared projections for Q, K, V\n",
    "        self.query_transform = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.key_transform = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.value_transform = nn.Linear(hidden_dim, attention_dim)\n",
    "        \n",
    "        # Output projections for forward and backward attention\n",
    "        self.output_forward = nn.Linear(attention_dim, hidden_dim)\n",
    "        self.output_backward = nn.Linear(attention_dim, hidden_dim)\n",
    "        \n",
    "        self.attention_scale = attention_dim ** -0.5\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim ),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.ff_j = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim ),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm4 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "\n",
    "        query = self.query_transform(x)\n",
    "        key = self.key_transform(x)\n",
    "        value = self.value_transform(x)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) * self.attention_scale\n",
    "        \n",
    "        # Create forward causal mask (lower triangular)\n",
    "        forward_mask = torch.tril(torch.ones(seq_length, seq_length)).bool().to(x.device)\n",
    "        \n",
    "        # Apply forward mask\n",
    "        forward_scores = attention_scores.masked_fill(~forward_mask, float('-10e7'))\n",
    "        forward_weights = F.softmax(forward_scores, dim=-1)\n",
    "        forward_attended = torch.matmul(forward_weights, value)\n",
    "        forward_output = self.output_forward(forward_attended)\n",
    "\n",
    "        # Process forward output for state prediction\n",
    "        x_forward = self.norm1(x + forward_output)\n",
    "        ff_output = self.ff(x_forward)\n",
    "        x_forward = self.norm2(x_forward + ff_output)\n",
    "               \n",
    "        return x_forward\n",
    "\n",
    "class StatePredictor(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, param_dim, num_steps=10):\n",
    "        super().__init__()\n",
    "        self.layers = num_steps\n",
    "        self.state_dim = state_dim\n",
    "        self.param_dim = param_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.state_transforms = nn.ModuleList([nn.Linear(state_dim, hidden_dim) for _ in range(num_steps)])\n",
    "        self.param_transform = nn.Linear(param_dim, hidden_dim ) \n",
    "        self.combine_transforms = nn.ModuleList([nn.Linear(hidden_dim * 2, hidden_dim) for _ in range(num_steps)])\n",
    "        \n",
    "        self.attention_layer = DualPurposeAttentionLayer(hidden_dim, attention_dim=hidden_dim // 2)\n",
    "    \n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(hidden_dim, 128) for _ in range(num_steps)])\n",
    "        self.out_layers = nn.ModuleList([nn.Linear(128, state_dim) for _ in range(num_steps)])\n",
    "\n",
    "        self.fc_layers_j = nn.ModuleList([nn.Linear(hidden_dim, 128) for _ in range(num_steps)])\n",
    "        self.out_layers_j = nn.ModuleList([nn.Linear(128, state_dim) for _ in range(num_steps)])\n",
    "\n",
    "        self.GELU = nn.GELU()\n",
    "        self.ELU = nn.ELU()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d_model):\n",
    "        position = torch.arange(seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, inputs, inputm):\n",
    "        batch_size = inputs.size(0)\n",
    "        latent_variables = []\n",
    "        current_state = inputs\n",
    "        current_backward_state = inputs\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            param_key = self.param_transform(inputm)\n",
    "            state_query = self.state_transforms[i](current_state)\n",
    "\n",
    "            combined = self.Tanh(self.combine_transforms[i](torch.cat([param_key, state_query], dim=-1)))\n",
    "            latent_variables.append(combined)\n",
    "\n",
    "        latent_stack = torch.stack(latent_variables, dim=1)\n",
    "        pe = self.get_positional_encoding(self.layers, self.hidden_dim).to(latent_stack.device)\n",
    "        latent_stack = latent_stack + pe\n",
    "\n",
    "        forward_attended = self.attention_layer(latent_stack)\n",
    "\n",
    "        forward_outputs = []\n",
    "        for i in range(self.layers):\n",
    "            s = self.ELU(self.fc_layers[i](forward_attended[:, i, :]))\n",
    "            new_state = self.out_layers[i](s) + current_state\n",
    "            forward_outputs.append(new_state)\n",
    "            current_state = new_state\n",
    "\n",
    "        jac_outputs = []\n",
    "        \n",
    "        # for i in reversed(range(self.layers)):\n",
    "        for i in range(self.layers):\n",
    "            s = self.ELU(self.fc_layers_j[i](forward_attended[:, i, :]))\n",
    "            j_output = self.out_layers_j[i](s) + current_backward_state\n",
    "            jac_outputs.append(j_output)  # Insert at the beginning to maintain original order\n",
    "            current_backward_state = j_output\n",
    "\n",
    "        forward_outputs_stacked = torch.stack(forward_outputs, dim=1)\n",
    "        jac_outputs_stacked = torch.stack(jac_outputs, dim=1)\n",
    "\n",
    "        return forward_outputs_stacked, jac_outputs_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attention layer for original version\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# class DualPurposeAttentionLayer(nn.Module):\n",
    "#     def __init__(self, hidden_dim, attention_dim):\n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.attention_dim = attention_dim\n",
    "        \n",
    "#         # Shared projections for Q, K, V\n",
    "#         self.query_transform = nn.Linear(hidden_dim, attention_dim,bias=False)\n",
    "#         self.key_transform = nn.Linear(hidden_dim, attention_dim,bias=False)\n",
    "#         self.value_transform = nn.Linear(hidden_dim, attention_dim,bias=False)\n",
    "        \n",
    "#         # Output projections for forward and backward attention\n",
    "#         self.output_forward = nn.Linear(attention_dim, hidden_dim)\n",
    "#         self.output_backward = nn.Linear(attention_dim, hidden_dim)\n",
    "        \n",
    "#         self.attention_scale = attention_dim ** -0.5\n",
    "\n",
    "#         self.ff = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim ),\n",
    "#             nn.ELU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim)\n",
    "#         )\n",
    "\n",
    "#         self.ff_j = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim ),\n",
    "#             nn.ELU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim)\n",
    "#         )\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "#         self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "#         self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "#         self.norm4 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         seq_length = x.size(1)\n",
    "\n",
    "#         query = self.query_transform(x)\n",
    "#         key = self.key_transform(x)\n",
    "#         value = self.value_transform(x)\n",
    "\n",
    "#         attention_scores = torch.matmul(query, key.transpose(-2, -1)) * self.attention_scale\n",
    "        \n",
    "#         # Create forward causal mask (lower triangular)\n",
    "#         forward_mask = torch.tril(torch.ones(seq_length, seq_length)).bool().to(x.device)\n",
    "        \n",
    "#         # Apply forward mask\n",
    "#         forward_scores = attention_scores.masked_fill(~forward_mask, float('-10e7'))\n",
    "#         forward_weights = F.softmax(forward_scores, dim=-1)\n",
    "#         forward_attended = torch.matmul(forward_weights, value)\n",
    "#         forward_output = self.output_forward(forward_attended)\n",
    "\n",
    "#         # Process forward output for state prediction\n",
    "#         x_forward = self.norm1(x + forward_output)\n",
    "#         ff_output = self.ff(x_forward)\n",
    "#         x_forward = self.norm2(x_forward + ff_output)\n",
    "               \n",
    "#         return x_forward\n",
    "\n",
    "# class StatePredictor(nn.Module):\n",
    "#     def __init__(self, state_dim, hidden_dim, param_dim, num_steps=10):\n",
    "#         super().__init__()\n",
    "#         self.layers = num_steps\n",
    "#         self.state_dim = state_dim\n",
    "#         self.param_dim = param_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.state_transforms = nn.ModuleList([nn.Linear(state_dim, hidden_dim) for _ in range(num_steps)])\n",
    "#         self.param_transform = nn.Linear(param_dim, hidden_dim ) \n",
    "#         self.combine_transforms = nn.ModuleList([nn.Linear(hidden_dim * 2, hidden_dim) for _ in range(num_steps)])\n",
    "        \n",
    "#         self.attention_layer = DualPurposeAttentionLayer(hidden_dim, attention_dim=hidden_dim // 2)\n",
    "    \n",
    "#         self.fc_layers = nn.ModuleList([nn.Linear(hidden_dim, 128) for _ in range(num_steps)])\n",
    "#         self.out_layers = nn.ModuleList([nn.Linear(128, state_dim) for _ in range(num_steps)])\n",
    "\n",
    "#         self.fc_layers_j = nn.ModuleList([nn.Linear(hidden_dim, 128) for _ in range(num_steps)])\n",
    "#         self.out_layers_j = nn.ModuleList([nn.Linear(128, state_dim) for _ in range(num_steps)])\n",
    "\n",
    "#         self.GELU = nn.GELU()\n",
    "#         self.ELU = nn.ELU()\n",
    "#         self.Tanh = nn.Tanh()\n",
    "\n",
    "#     def get_positional_encoding(self, seq_len, d_model):\n",
    "#         position = torch.arange(seq_len).unsqueeze(1).float()\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(seq_len, d_model)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "#         return pe.unsqueeze(0)\n",
    "\n",
    "#     def forward(self, inputs, inputm):\n",
    "#         batch_size = inputs.size(0)\n",
    "#         latent_variables = []\n",
    "#         current_state = inputs\n",
    "#         current_backward_state = inputs\n",
    "\n",
    "#         for i in range(self.layers):\n",
    "#             param_key = self.param_transform(inputm)\n",
    "#             state_query = self.state_transforms[i](current_state)\n",
    "\n",
    "#             combined = self.Tanh(self.combine_transforms[i](torch.cat([param_key, state_query], dim=-1)))\n",
    "#             latent_variables.append(combined)\n",
    "\n",
    "#         latent_stack = torch.stack(latent_variables, dim=1)\n",
    "#         pe = self.get_positional_encoding(self.layers, self.hidden_dim).to(latent_stack.device)\n",
    "#         latent_stack = latent_stack + pe\n",
    "\n",
    "#         forward_attended = self.attention_layer(latent_stack)\n",
    "\n",
    "#         forward_outputs = []\n",
    "#         for i in range(self.layers):\n",
    "#             s = self.ELU(self.fc_layers[i](forward_attended[:, i, :]))\n",
    "#             new_state = self.out_layers[i](s) + current_state\n",
    "#             forward_outputs.append(new_state)\n",
    "#             current_state = new_state\n",
    "\n",
    "#         jac_outputs = []\n",
    "        \n",
    "#         # for i in reversed(range(self.layers)):\n",
    "#         for i in range(self.layers):\n",
    "#             s = self.ELU(self.fc_layers_j[i](forward_attended[:, i, :]))\n",
    "#             j_output = self.out_layers_j[i](s) + current_backward_state\n",
    "#             jac_outputs.append(j_output)  # Insert at the beginning to maintain original order\n",
    "#             current_backward_state = j_output\n",
    "\n",
    "#         forward_outputs_stacked = torch.stack(forward_outputs, dim=1)\n",
    "#         jac_outputs_stacked = torch.stack(jac_outputs, dim=1)\n",
    "\n",
    "#         return forward_outputs_stacked, jac_outputs_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian_batched(model, inputs, params):\n",
    "    def single_jacobian(inp, param):\n",
    "        def get_backward_output(p):\n",
    "            _, backward_output = model(inp.unsqueeze(0), p.unsqueeze(0))\n",
    "            return backward_output.squeeze(0)\n",
    "        return functorch.jacfwd(get_backward_output)(param)\n",
    "\n",
    "    return functorch.vmap(single_jacobian)(inputs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint directory\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training components\n",
    "model = StatePredictor(129, 100, 128, num_steps=num_steps)  # state_dim=129, hidden_dim=100, param_dim=128\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)  # Reduce LR when loss plateaus\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_jac_losses = []\n",
    "    \n",
    "    for inputm, inputs, targets, Jacs in train_loader:\n",
    "        # Move data to device\n",
    "        inputm, inputs, targets, Jacs = inputm.to(device), inputs.to(device), targets.to(device), Jacs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        forward_outputs, backward_outputs = model(inputs, inputm)\n",
    "\n",
    "        # Compute forward prediction loss for each time step\n",
    "        step_losses = [loss_function(forward_outputs[:, i], targets[:, i]) for i in range(num_steps)]\n",
    "        main_loss = sum(step_losses)\n",
    "        \n",
    "        # Jacobian loss weight\n",
    "        lambda_jac = 1.\n",
    "        \n",
    "        # Compute Jacobian using automatic differentiation\n",
    "        jacobian_func = compute_jacobian_batched(model, inputs, inputm)\n",
    "        jac_losses = [torch.norm(jacobian_func[:, i] - Jacs[:, i], dim=(1, 2)).mean() for i in range(num_steps)]\n",
    "        jac_loss_total = sum(jac_losses)\n",
    "        \n",
    "        # Total loss combines forward prediction and Jacobian accuracy\n",
    "        loss = main_loss + jac_loss_total\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_jac_losses.append(jac_loss_total.item())\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = [0] * num_steps\n",
    "    val_jac_losses = [0] * num_steps\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputm, inputs, targets, Jacs in val_loader:\n",
    "            inputm, inputs, targets, Jacs = inputm.to(device), inputs.to(device), targets.to(device), Jacs.to(device)\n",
    "            forward_outputs, backward_outputs = model(inputs, inputm)\n",
    "\n",
    "            # Accumulate validation losses for each time step\n",
    "            for i in range(num_steps):\n",
    "                val_losses[i] += loss_function(forward_outputs[:, i], targets[:, i]).item()\n",
    "\n",
    "            # Compute validation Jacobian losses\n",
    "            jacobian_func = compute_jacobian_batched(model, inputs, inputm)\n",
    "            jac_losses = [torch.norm(jacobian_func[:, i] - Jacs[:, i], dim=(1, 2)).mean() for i in range(num_steps)]\n",
    "            for i in range(num_steps):\n",
    "                val_jac_losses[i] += jac_losses[i].item()\n",
    "\n",
    "    # Average validation losses across batches\n",
    "    avg_val_losses = [loss / len(val_loader) for loss in val_losses]\n",
    "    avg_val_jac_losses = [loss / len(val_loader) for loss in val_jac_losses]\n",
    "    avg_total_loss = sum(avg_val_losses) / num_steps\n",
    "    avg_total_jac_loss = sum(avg_val_jac_losses) / num_steps\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(avg_total_loss + avg_total_jac_loss)\n",
    "\n",
    "    # Print detailed loss information\n",
    "    loss_string = ', '.join(f'Step {i+1}: {loss:.4f}' for i, loss in enumerate(avg_val_losses))\n",
    "    jac_loss_string = ', '.join(f'Jac Step {i+1}: {loss:.4f}' for i, loss in enumerate(avg_val_jac_losses))\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'  Main Loss: {loss_string}, Avg Main Loss: {avg_total_loss:.4f}')\n",
    "    print(f'  Jac Loss: {jac_loss_string}, Avg Jac Loss: {avg_total_jac_loss:.4f}')\n",
    "\n",
    "    total_val_loss = avg_total_loss + avg_total_jac_loss\n",
    "    \n",
    "    # Early stopping and model checkpointing\n",
    "    if total_val_loss < best_val_loss:\n",
    "        best_val_loss = total_val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'num_steps': num_steps,\n",
    "            'patience_counter': patience_counter,\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'best_model_checkpoint_100_casual_jac.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved best model checkpoint to {checkpoint_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "m_test = m_red[-100:]\n",
    "m_test_r = ((m_test - mean)/std).to(dtype=torch.float32)\n",
    "\n",
    "test_y_in = torch.tensor(u_array_[-100:,0,:]@PCA, dtype=torch.float32)\n",
    "test_y_out = torch.tensor(u_array[-100:,1:1+num_steps,:], dtype=torch.float32)\n",
    "\n",
    "test_t_in_r = (test_y_in-mean_o)/std_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model = model.to('cpu')\n",
    "m_test_r = m_test_r.to('cpu')\n",
    "PCA_ = torch.tensor(PCA, dtype=torch.float32)\n",
    "states = model(test_t_in_r, m_test_r)\n",
    "obs_mean_t = torch.tensor(obs_mean,dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(model, test_t_in_r, m_test_r, test_y_out, std_o, mean_o, PCA_, M, obs_mean_t):\n",
    "    # Generate states from the model\n",
    "    with torch.no_grad():\n",
    "        states = model(test_t_in_r, m_test_r)[0]\n",
    "    \n",
    "    # Adjust and transform states in a vectorized manner\n",
    "    adjusted_states = (states * std_o + mean_o) @ PCA_.float().T\n",
    "    \n",
    "    # Reshape test_y_out and adjusted_states for broadcasting\n",
    "    test_y_out = test_y_out.view(100, 100, -1)\n",
    "    adjusted_states = adjusted_states.view(100, 100, -1)\n",
    "    \n",
    "    # Compute differences\n",
    "    diff = test_y_out - adjusted_states - obs_mean_t\n",
    "    \n",
    "    # Compute norms using einsum for efficiency\n",
    "    norm_adjusted = torch.einsum('ijk,kl,ijl->ij', diff, M, diff)\n",
    "    norm_test = torch.einsum('ijk,kl,ijl->ij', test_y_out, M, test_y_out)\n",
    "    \n",
    "    # Compute errors\n",
    "    errors = torch.sqrt(norm_adjusted) / torch.sqrt(norm_test)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = compute_errors(model.to('cpu'), test_t_in_r.to('cpu'), m_test_r.to('cpu'), test_y_out.to('cpu'), std_o.to('cpu'), mean_o.to('cpu'), PCA_.to('cpu'), M.to('cpu'), obs_mean_t.to('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0213, 0.0416, 0.0554,  ..., 0.0197, 0.0198, 0.0200],\n",
       "        [0.0222, 0.0315, 0.0448,  ..., 0.0164, 0.0172, 0.0163],\n",
       "        [0.0320, 0.0506, 0.0747,  ..., 0.0236, 0.0246, 0.0232],\n",
       "        ...,\n",
       "        [0.0600, 0.0954, 0.1054,  ..., 0.0418, 0.0425, 0.0391],\n",
       "        [0.0218, 0.0355, 0.0451,  ..., 0.0196, 0.0188, 0.0191],\n",
       "        [0.0367, 0.0556, 0.0586,  ..., 0.0176, 0.0164, 0.0163]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_func = compute_jacobian_batched(model,test_t_in_r,m_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jac_test = Jac_r[-100:]\n",
    "mean_normalized_differences = []\n",
    "Jac_comp = jacobian_func.to('cpu') / std.unsqueeze(0).unsqueeze(0) * std_o.unsqueeze(1).unsqueeze(0)\n",
    "for i in range(100):\n",
    "    normalized_diff = (Jac_comp[:,i] - Jac_test[:, i]).norm(dim=(1, 2)) / Jac_test[:, i].norm(dim=(1, 2))\n",
    "    mean_normalized_differences.append(normalized_diff.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.1116, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0732, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0567, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0484, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0426, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0431, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0394, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0361, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0338, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0341, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0318, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0303, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0287, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0295, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0297, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0283, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0270, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0271, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0281, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0251, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0246, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0244, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0246, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0245, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0255, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0242, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0237, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0249, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0240, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0238, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0248, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0218, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0220, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0220, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0227, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0226, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0236, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0226, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0223, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0214, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0215, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0217, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0218, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0225, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0228, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0232, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0223, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0218, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0228, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0233, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0234, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0232, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0233, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0231, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0227, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0227, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0226, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0216, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0212, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0227, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0225, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0242, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0251, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0247, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0221, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0202, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0211, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0210, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0216, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0207, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0206, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0206, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0203, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0214, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0200, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0189, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0201, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0200, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0182, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0180, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0181, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0188, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0180, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0173, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0165, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0169, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0172, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0168, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0164, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0161, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0162, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0167, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0168, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0166, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0167, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0158, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0158, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0161, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0162, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0170, dtype=torch.float64, grad_fn=<MeanBackward0>)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_normalized_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for MAP point estimation\n",
    "map_m = torch.tensor(np.load('combined_map_m_100_10.npy'),dtype=torch.float64)  \n",
    "init_y = torch.tensor(sdata_y[0].clone().detach().unsqueeze(0).repeat(200, 1),dtype=torch.float32) \n",
    "map_obs = torch.tensor(np.load('combined_map_obs_100_10.npy'),dtype=torch.float64) \n",
    "map_map = torch.tensor(np.load('combined_map_map_100_10.npy'),dtype=torch.float64)\n",
    "\n",
    "# Prepare MAP parameter data\n",
    "map_m_ = map_m - prior_mean  \n",
    "map_m_red = map_m_ @ (prior_m_precision @ AS)  \n",
    "map_prior = ((map_m_red - mean)/std).float()  \n",
    "\n",
    "# Prepare MAP observation data\n",
    "map_obs_ = map_obs[:,1:]  \n",
    "map_obs_2 = map_obs_ - obs_mean  \n",
    "\n",
    "# Noise variance for uncertainty quantification\n",
    "noise_var=3.9e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute matrices for efficient L-BFGS optimization\n",
    "UNoise_precU = PCA_.double().T @ Noise_prec @ PCA_.double()  \n",
    "yMU = (obs_w_noise[:, 9::10] @ Noise_prec @ PCA_.double())   \n",
    "\n",
    "# Set up L-BFGS optimizer for quasi-Newton optimization\n",
    "model.eval()\n",
    "optimizer = optim.LBFGS([m], \n",
    "                       lr=learning_rate, \n",
    "                       max_iter=150,           \n",
    "                       max_eval=None,          \n",
    "                       tolerance_grad=1e-7,    \n",
    "                       tolerance_change=1e-9,  \n",
    "                       history_size=150,       \n",
    "                       line_search_fn=\"strong_wolfe\") \n",
    "\n",
    "# Storage for optimization history\n",
    "L2_lbfgs = []\n",
    "\n",
    "def closure():\n",
    "    \"\"\"Closure function for L-BFGS optimizer - computes loss and gradients\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass through neural network\n",
    "    states = model(init_y, m)[0]\n",
    "    \n",
    "    # Transform predictions to observation space\n",
    "    nn_val = (states*std_o + mean_o).double()[:,9::10]  \n",
    "    \n",
    "    # Efficient likelihood computation using precomputed matrices\n",
    "    like1 = torch.einsum('bij,bij->b',nn_val,yMU)                  \n",
    "    like2 = torch.einsum('bij,jk,bik->b', nn_val, UNoise_precU, nn_val)  \n",
    "    like = (-2*like1 + like2)/2.                                 \n",
    "    \n",
    "    # Prior regularization term\n",
    "    prior = torch.einsum('ij,ij->i', m, m) /2.\n",
    "    \n",
    "    # Total loss (negative log posterior)\n",
    "    loss = (prior + like).mean()\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Optimization loop\n",
    "for epoch in range(1):\n",
    "    loss = optimizer.step(closure)  \n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Store optimization metrics\n",
    "    L2_lbfgs.append(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
