{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jgo31/anaconda3/envs/fe_pt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import simulation data\n",
    "u_array = np.load('unp_concat_100.npy')\n",
    "m_array = np.load('mnp_concat_100.npy')\n",
    "\n",
    "# Import reduced-order subspaces\n",
    "AS = np.load('AS_fs_wom.npy')\n",
    "PCA = np.load('PCA_U_f_100_allstep.npy')\n",
    "\n",
    "# Import finite element mass matrix\n",
    "M = np.load('M.npy')\n",
    "\n",
    "# Import Bayesian prior information\n",
    "prior_m_precision = np.load('prior_prec.npy')\n",
    "prior_m_covariance = np.load('prior.npy')\n",
    "prior_mean = np.load('prior_mean.npy')\n",
    "\n",
    "# Import reference solution data\n",
    "obs_mean = np.load('obs_mean.npy')\n",
    "\n",
    "# Prepare parameter data\n",
    "m_array = torch.tensor(m_array)\n",
    "m_array_ = m_array - prior_mean  \n",
    "\n",
    "# Reduce parameter dimension using active subspace\n",
    "m_red = m_array_ @ (prior_m_precision @ AS)\n",
    "\n",
    "# Prepare observation data\n",
    "u_array_ = u_array - obs_mean \n",
    "\n",
    "# Truncate PCA basis to 129 dimensions\n",
    "PCA = PCA[:,:129]\n",
    "\n",
    "# Set number of time steps for training\n",
    "num_steps = 100\n",
    "\n",
    "# Create training datasets\n",
    "train_m = torch.tensor(m_red.numpy()[:-100], dtype=torch.float32)  \n",
    "train_s = torch.tensor(u_array_[:-100,0,:]@PCA, dtype=torch.float32)  \n",
    "output_s = torch.tensor(u_array_[:-100,1:1+num_steps,:]@PCA, dtype=torch.float32) \n",
    "\n",
    "# Normalize parameter data\n",
    "train_m_100 = train_m\n",
    "\n",
    "mean = torch.mean(train_m_100,dim=0)  \n",
    "std = torch.std(train_m_100,dim=0)    \n",
    "\n",
    "sdata_m = (train_m_100 - mean)/std   \n",
    "\n",
    "# Normalize observation data\n",
    "mean_o = output_s.mean(dim = (0,1))   \n",
    "# std_o = output_s.std(dim=(0,1))     \n",
    "\n",
    "# Set observation std to 1 for normalization\n",
    "std_o = torch.ones_like(std_o)\n",
    "\n",
    "sdata_y = (train_s - mean_o)/(std_o)      \n",
    "sdata_y_t = (output_s - mean_o)/(std_o)  \n",
    "\n",
    "# Limit dataset size to 1280 samples\n",
    "sdata_m_ = sdata_m[:1024+256]\n",
    "sdata_y_ = sdata_y[:1024+256]\n",
    "sdata_y_t_ = sdata_y_t[:1024+256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "def load_checkpoint_and_inspect(checkpoint_path, model):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    num_steps = checkpoint['num_steps']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "    \n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        print(\"Optimizer state in checkpoint:\")\n",
    "        print(checkpoint['optimizer_state_dict'].keys())\n",
    "        for param_group in checkpoint['optimizer_state_dict']['param_groups']:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "            print(f\"Parameters: {len(param_group['params'])}\")\n",
    "    else:\n",
    "        print(\"No optimizer state found in checkpoint\")\n",
    "    \n",
    "    return model, epoch, best_val_loss, num_steps, patience_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer state in checkpoint:\n",
      "dict_keys(['state', 'param_groups'])\n",
      "Learning rate: 0.005\n",
      "Parameters: 1\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and checkpoint data\n",
    "checkpoint_path = 'checkpoints/best_model_checkpoint_good_jac.pth'\n",
    "model = StatePredictor(129, 100, 128, num_steps=num_steps)\n",
    "model, start_epoch, best_val_loss, num_steps, patience_counter = load_checkpoint_and_inspect(checkpoint_path, model)\n",
    "\n",
    "# Create a fresh optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1664628/2078478244.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  init_y = torch.tensor(sdata_y[0].clone().detach().unsqueeze(0).repeat(200, 1),\n"
     ]
    }
   ],
   "source": [
    "# Move normalization statistics to device\n",
    "std_o = std_o.to(device)\n",
    "std = std.to(device)\n",
    "mean_o = mean_o.to(device)\n",
    "sdata_m = sdata_m.to(device)\n",
    "PCA_ = torch.tensor(PCA, device=device)\n",
    "noise_var = 3.9e-3\n",
    "Noise_prec = (M.double() / noise_var).to(device)\n",
    "\n",
    "init_y = torch.tensor(sdata_y[0].clone().detach().unsqueeze(0).repeat(200, 1), \n",
    "                      dtype=torch.float32, device=device)\n",
    "UMU = PCA_.T @ Noise_prec @ PCA_\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Design for initial design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Generate all combinations of 4 elements\n",
    "combinations = list(itertools.combinations(numbers, 4))\n",
    "\n",
    "for combo in combinations:\n",
    "    print('current_comb', combo)\n",
    "    combo_list = list(combo)\n",
    "    \n",
    "    ## Generate prior sample\n",
    "    random_indices = torch.randperm(1024)[:200]\n",
    "    m_samples = sdata_m[random_indices]\n",
    "    states = model(init_y,m_samples)[0]\n",
    "    full_states = (states*std_o + mean_o).double() @ PCA_.T\n",
    "    noise = torch.normal(mean=0, std=np.sqrt(noise_var), size=full_states.shape).to(device)\n",
    "    full_noisy_states = (noise + full_states).detach()\n",
    "\n",
    "    ## Precompute\n",
    "    yMU = (full_noisy_states[:,combo_list] @ Noise_prec @ PCA_)\n",
    "\n",
    "    ## MAP point\n",
    "    num_iterations = 0\n",
    "    learning_rate = 0.005\n",
    "    number_of_samples = 200\n",
    "    model.eval()\n",
    "    L2_lbfgs = []\n",
    "    m = m_samples.detach().clone().requires_grad_(True)\n",
    "    optimizer = optim.LBFGS([m], \n",
    "                       lr=learning_rate, \n",
    "                       max_iter=150,           \n",
    "                       max_eval=None,          \n",
    "                       tolerance_grad=1e-7,    \n",
    "                       tolerance_change=1e-9,  \n",
    "                       history_size=150,       \n",
    "                       line_search_fn=\"strong_wolfe\") \n",
    "\n",
    "    def closure():\n",
    "        \"\"\"Closure function for L-BFGS optimizer - computes loss and gradients\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through neural network\n",
    "        states = model(init_y, m)[0]\n",
    "        \n",
    "        # Transform predictions to observation space\n",
    "        nn_val = (states*std_o + mean_o).double()[:,9::10]  \n",
    "        \n",
    "        # Efficient likelihood computation using precomputed matrices\n",
    "        like1 = torch.einsum('bij,bij->b',nn_val,yMU)                  \n",
    "        like2 = torch.einsum('bij,jk,bik->b', nn_val, UNoise_precU, nn_val)  \n",
    "        like = (-2*like1 + like2)/2.                                 \n",
    "        \n",
    "        # Prior regularization term\n",
    "        prior = torch.einsum('ij,ij->i', m, m) /2.\n",
    "        \n",
    "        # Total loss (negative log posterior)\n",
    "        loss = (prior + like).mean()\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimization loop\n",
    "    for epoch in range(1):\n",
    "        loss = optimizer.step(closure)  \n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Store optimization metrics\n",
    "        L2_lbfgs.append(loss.item())\n",
    "\n",
    "    ## Compute Jacobian at the Map point\n",
    "    jacobian_func = compute_jacobian_batched(model, init_y, m.detach())\n",
    "    with torch.no_grad():\n",
    "        jacobian_func_t = jacobian_func.transpose(2, 3)\n",
    "        res = torch.einsum('abij,jk,abkl->abil', jacobian_func_t.double(), UMU, jacobian_func.double())\n",
    "\n",
    "        ## Compute the JTNJ\n",
    "        res1= res.mean(dim=1)\n",
    "        eigenvalues = torch.linalg.eigvalsh((res1))\n",
    "\n",
    "        l1 = torch.log(1+ eigenvalues).sum(dim=1)\n",
    "        l2 = (-eigenvalues/(1+eigenvalues)).sum(dim=1)\n",
    "        map_prior = m.detach()\n",
    "        l3 = torch.sum(map_prior * map_prior, dim=1)\n",
    "\n",
    "        beig = l1+l2+l3\n",
    "        cur_eig = beig.mean().detach()\n",
    "        eig_list.append(cur_eig)\n",
    "        if cur_eig > max_eig:\n",
    "            print('max_eig', cur_eig)\n",
    "            max_eig = cur_eig\n",
    "            best_comb = combo\n",
    "            print(\"best_comb\", combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eig = -1\n",
    "eig_list = []\n",
    "N = 10\n",
    "xi_star = np.zeros(N, dtype=int)\n",
    "dy_star = np.zeros(N)\n",
    "Y_real = {}\n",
    "noise_std = np.sqrt(noise_var)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate true observation\n",
    "\n",
    "random_indices_true = torch.randint(0, 100, (1,))\n",
    "print(random_indices_true)\n",
    "# random_indices_true = 84\n",
    "\n",
    "model = model.to('cpu')\n",
    "states_true = model(test_t_in_r[random_indices_true], m_test_r[random_indices_true])[0].double()\n",
    "\n",
    "# Adjust each state and transform\n",
    "adjusted_states_true = torch.stack([(state * std_o.to('cpu') + mean_o.to('cpu')) @ PCA_.to('cpu').T for state in states_true]).detach()\n",
    "\n",
    "noise = torch.normal(mean=0, std=noise_std, size=adjusted_states_true.shape)\n",
    "full_noisy_true_states = (noise + adjusted_states_true).detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_post(post_map, post_eigval, post_eigvec, device=device):\n",
    "    s = 1-1/torch.sqrt(post_eigval+1)\n",
    "    return post_map + torch.rand(200, len(s)).double().to(device)@(torch.eye(len(s)).to(device) - post_eigvec @ torch.diag(s) @ post_eigvec.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "next_time_to_check = 2\n",
    "best_indices = [1]\n",
    "\n",
    "cur_eig = 0\n",
    "max_eig = 0\n",
    "\n",
    "# Initialize true_obs with the correct shape\n",
    "true_obs = torch.zeros_like(full_noisy_true_states).to(device)\n",
    "\n",
    "m1 = nn.Parameter(torch.zeros((1,128)).to(device))\n",
    "init_y1 = init_y[0].unsqueeze(0)\n",
    "num_iterations = 0\n",
    "learning_rate = 0.005\n",
    "number_of_samples = 200\n",
    "\n",
    "\n",
    "optimizer = optim.LBFGS([m1], \n",
    "                       lr=learning_rate, \n",
    "                       max_iter=150,           \n",
    "                       max_eval=None,          \n",
    "                       tolerance_grad=1e-7,    \n",
    "                       tolerance_change=1e-9,  \n",
    "                       history_size=150,       \n",
    "                       line_search_fn=\"strong_wolfe\") \n",
    "\n",
    "    def closure():\n",
    "        \"\"\"Closure function for L-BFGS optimizer - computes loss and gradients\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through neural network\n",
    "        states = model(init_y, m1)[0]\n",
    "        \n",
    "        # Transform predictions to observation space\n",
    "        nn_val = (states*std_o + mean_o).double()[:,9::10]  \n",
    "        \n",
    "        # Efficient likelihood computation using precomputed matrices\n",
    "        like1 = torch.einsum('bij,bij->b',nn_val,yMU)                  \n",
    "        like2 = torch.einsum('bij,jk,bik->b', nn_val, UNoise_precU, nn_val)  \n",
    "        like = (-2*like1 + like2)/2.                                 \n",
    "        \n",
    "        # Prior regularization term\n",
    "        prior = torch.einsum('ij,ij->i', m1, m1) /2.\n",
    "        \n",
    "        # Total loss (negative log posterior)\n",
    "        loss = (prior + like).mean()\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimization loop\n",
    "    for epoch in range(1):\n",
    "        loss = optimizer.step(closure)  \n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Store optimization metrics\n",
    "        L2_lbfgs.append(loss.item())\n",
    "\n",
    "jacobian_func1 = compute_jacobian_batched(model, init_y1, m1.detach())\n",
    "with torch.no_grad():\n",
    "    jacobian_func1_t = jacobian_func1.transpose(2, 3)\n",
    "    res_t2 = torch.einsum('abij,jk,abkl->abil', jacobian_func1_t.double(), UMU, jacobian_func1.double())\n",
    "\n",
    "    ## Compute the JTNJ\n",
    "    gn_h = res_t2[0,best_indices].mean(dim=0)\n",
    "\n",
    "    eigvalues, eigenectors  = torch.linalg.eigh((gn_h))\n",
    "    eigvalues = eigvalues.flip(0)\n",
    "    eigenectors = eigenectors.flip(1)\n",
    "\n",
    "post_map, post_eigval, post_eigvec = m1.detach(), eigvalues, eigenectors\n",
    "\n",
    "for k in range(1,3):\n",
    "    max_eig = float('-inf')\n",
    "    bestIndex = -1\n",
    "\n",
    "    # Generate all combinations of 4 elements\n",
    "    combinations = list(itertools.combinations(numbers[next_time_to_check:], 4-k))\n",
    "\n",
    "    for combo in combinations:\n",
    "        print('current_comb', combo)\n",
    "        combo_list = best_indices + list(combo)\n",
    "        ## Generate prior sample\n",
    "        random_indices = torch.randperm(1024)[:200]\n",
    "        m_samples = sdata_m[random_indices]\n",
    "        states = model(init_y,m_samples)[0]\n",
    "        full_states = (states*std_o + mean_o).double() @ PCA_.T\n",
    "        noise = torch.normal(mean=0, std=np.sqrt(noise_var), size=full_states.shape).to(device)\n",
    "        full_noisy_states = (noise + full_states).detach()\n",
    "\n",
    "        if best_indices:\n",
    "            full_noisy_states[:, best_indices] = true_obs[:, best_indices]\n",
    "\n",
    "        ## Precompute\n",
    "        yMU = (full_noisy_states[:,combo_list] @ Noise_prec @ PCA_)\n",
    "\n",
    "        ## MAP point\n",
    "        num_iterations = 0\n",
    "        learning_rate = 0.005\n",
    "        number_of_samples = 200\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        optimizer = optim.LBFGS([m], \n",
    "                       lr=learning_rate, \n",
    "                       max_iter=150,           \n",
    "                       max_eval=None,          \n",
    "                       tolerance_grad=1e-7,    \n",
    "                       tolerance_change=1e-9,  \n",
    "                       history_size=150,       \n",
    "                       line_search_fn=\"strong_wolfe\") \n",
    "\n",
    "        # Storage for optimization history\n",
    "        L2_lbfgs = []\n",
    "\n",
    "        def closure():\n",
    "            \"\"\"Closure function for L-BFGS optimizer - computes loss and gradients\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through neural network\n",
    "            states = model(init_y, m)[0]\n",
    "            \n",
    "            # Transform predictions to observation space\n",
    "            nn_val = (states*std_o + mean_o).double()[:,9::10]  \n",
    "            \n",
    "            # Efficient likelihood computation using precomputed matrices\n",
    "            like1 = torch.einsum('bij,bij->b',nn_val,yMU)                  \n",
    "            like2 = torch.einsum('bij,jk,bik->b', nn_val, UNoise_precU, nn_val)  \n",
    "            like = (-2*like1 + like2)/2.                                 \n",
    "            \n",
    "            # Prior regularization term\n",
    "            prior = torch.einsum('ij,ij->i', m, m) /2.\n",
    "            \n",
    "            # Total loss (negative log posterior)\n",
    "            loss = (prior + like).mean()\n",
    "            \n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        # Optimization loop\n",
    "        for epoch in range(1):\n",
    "            loss = optimizer.step(closure)  \n",
    "            \n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Store optimization metrics\n",
    "            L2_lbfgs.append(loss.item())\n",
    "\n",
    "        ## Compute Jacobian at the Map point\n",
    "        jacobian_func = compute_jacobian_batched(model, init_y, m.detach())\n",
    "        with torch.no_grad():\n",
    "            jacobian_func_t = jacobian_func.transpose(2, 3)\n",
    "            # print(jacobian_func_t.shape)\n",
    "            res = torch.einsum('abij,jk,abkl->abil', jacobian_func_t.double(), UMU, jacobian_func.double())\n",
    "            # print(res.shape)\n",
    "\n",
    "            ## Compute the JTNJ\n",
    "            res1= res.mean(dim=1)\n",
    "            # print(res1.shape)\n",
    "            eigenvalues = torch.linalg.eigvalsh((res1))\n",
    "\n",
    "            l1 = torch.log(1+ eigenvalues).sum(dim=1)\n",
    "            l2 = (-eigenvalues/(1+eigenvalues)).sum(dim=1)\n",
    "            map_prior = m.detach()\n",
    "            l3 = torch.sum(map_prior * map_prior, dim=1)\n",
    "\n",
    "            beig = l1+l2+l3\n",
    "            cur_eig = beig.mean().detach()\n",
    "            eig_list.append(cur_eig)\n",
    "            print(cur_eig)\n",
    "            if cur_eig > max_eig:\n",
    "                print('max_eig', cur_eig)\n",
    "                max_eig = cur_eig\n",
    "                best_comb = combo\n",
    "                print(\"best_comb\", combo)\n",
    "    \n",
    "    bestIndex = combo[k]\n",
    "    best_indices.append(bestIndex)\n",
    "    next_time_to_check = bestIndex + 1\n",
    "    true_obs[0, bestIndex] = full_noisy_true_states[0, bestIndex]\n",
    "    print(true_obs)\n",
    "    print(\"Done\")\n",
    "    print(\"bestIndex\",bestIndex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
